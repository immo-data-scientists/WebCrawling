---
title: "Web Crawling in R"
author: "SKY/VAM"
date: "04 February 2019"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: true
      tocdepth: 3
---

```{r, echo=FALSE, warning=FALSE,message=FALSE}
load(file = 'E:/MY_Project_Crawling/RWebCrawling/carDekho.RData')
```

## Scope of the study
In today's world, it is easy to find any kind of data from internet resources.The biggest challenge we face is in accessing it. Here, we aim to gain an overall idea on webscrapping using R.

## Inspiration
Recent manual work of data collection which was very tedious motivated to explore a bit in this area. Also this will help to obtain offered data from online sources in a much faster way.

## Pre requisites
* *`rvest`* package - Extracts information directly from webpages.
* *`Rcrawler`* package - Crawls and extracts information from web pages.
* SelectorGadeget - Extracts CSS (Cascading Style Sheets) patterns from the  webpages. We've used the chrome extension of selectorGadget.

## Website used
https://www.cardekho.com/
  - One of India's leading autoportal for cars.
  
## *`rvest`* 

### Basic usage
```{r, echo=TRUE, warning=FALSE,message=FALSE}
# initialisation
library(rvest)
library(tidyverse)

# url
urlcardekho <- 'https://www.cardekho.com/used-cars+in+bangalore'
```

### Example 1 : Single column
```{r, echo=TRUE, warning=FALSE,message=FALSE}
model <- html_session(urlcardekho) %>%
    html_nodes('.title a') %>% 
    html_text() %>%  
    str_replace_all( "[\r\n\t\\s\\/]" , "")

head(model)
```

### Example 2 : Multilpe columns
```{r, echo=TRUE, warning=FALSE,message=FALSE}
# function to scrap a column
scrapeCol <- function(Xpath, urlsite){
  dat <- html_session(urlsite) %>%
    html_nodes(Xpath) %>% 
    html_text() %>%  
    str_replace_all( "[\r\n\t\\s\\/]" , "")
}

# getting information from the webpage
cssPat <- c(model = '.title a', 
            price = '.price', 
            drivenKm = '.dotlist span:nth-child(1)', 
            fuel = '.dotlist span:nth-child(2)')

carData <- as.data.frame(lapply(cssPat, scrapeCol, urlcardekho))

DT:::datatable(carData, 
               rownames = FALSE,
               options = list(dom = 't'))
```

## *`RCrawler`*
RCrawler is an R-based web crawler that can crawl and scrape web page content (articles, titles, and metadata) in an automated manner to produce a structured dataset.By providing only the website URL and the Xpath or CSS selector patterns, `Rcrawler{RCrawler}` can crawl the whole website (traverse all web pages), download webpages, and scrape/extract its contents in an automated manner to produce a structured dataset.


<br> _Note: The process of a crawling operation is performed by several concurrent processes in parallel, so it's recommended to use 64bit version of R._

### Basic Usage
```{r, echo=TRUE, eval = FALSE, warning=FALSE,message=FALSE}

# preparing CSS config table
configCSS <- read.table(textConnection("
CSSPattern;PatternNames
.priceheader;price
.icon-cd-year+ .iconDetail;year
.kmDrive+ .iconDetail;kmDriven
.fuelType+ .iconDetail;fuel
.sellerType+ .iconDetail;seller
.icon-cd-Transmission+ .iconDetail;transmission
.ownerFirst+ .iconDetail;owner
.milage+ .iconDetail;milage
.engine+ .iconDetail;engine
.seats+ .iconDetail;seatNb
.borderBottom:nth-child(1) .on;info"), sep = ';', header = T, stringsAsFactors = F)

```

```{r, echo=FALSE, warning=FALSE,message=FALSE}
DT:::datatable(configCSS, rownames = FALSE, 
               options = list(dom = 't'))
```


```{r, echo=TRUE, eval=FALSE, warning=FALSE,message=FALSE}
# library
library(Rcrawler)

# crawling a website
Rcrawler(Website = "https://www.cardekho.com/" , 
         no_cores = 4, 
         crawlUrlfilter = 'used-car-details',
         ExtractCSSPat = configCSS$CSSPattern,
         PatternsNames = configCSS$PatternNames)
```


### `Rcrawler{RCrawler}` output
The output of Rcrawler() is as follows:

* **Repository**: Rcrawler saves the following to the working directory by default. We also have the provision to change the default directory to any desired one using the `DIR` parameterin Rcrawler().
   - Crawled webpages: Named as per the id column in `INDEX`
   - extracted_data.csv: csv file holding all extracted data.

* **`INDEX`**: A data frame in global environment representing the generic URL index.
    - fetched URLs
    - content type
    - encoding type
    - HTTP resp:codes indicate whether a specific HTTP request has been successfully completed(200 - ok).
    - number of inlinks:links from other pages that point to that page.
    - number of outlinks:number of links extracted and filtered from that page. 
    - level: it represents the distance between the root document and all extracted links.

* **`DATA`**: A list of lists in global environement holding scraped contents.

## Further scope
1. There are a lot of functions in crawler package which are yet to be explored.







 
